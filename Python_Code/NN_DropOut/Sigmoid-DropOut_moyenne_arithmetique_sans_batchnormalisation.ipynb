{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-27T15:04:29.693601Z",
     "iopub.status.busy": "2022-04-27T15:04:29.693321Z",
     "iopub.status.idle": "2022-04-27T15:04:29.700761Z",
     "shell.execute_reply": "2022-04-27T15:04:29.700069Z",
     "shell.execute_reply.started": "2022-04-27T15:04:29.693573Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import numpy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, add, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Nadam\n",
    "from keras.layers.core import Dropout, Activation\n",
    "from keras.initializers import RandomNormal, Ones\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import random\n",
    "from numpy import array\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_dist(X, model, num_samples):\n",
    "    preds = [model(X, training=True) for _ in range(num_samples)]\n",
    "    return np.hstack(preds)\n",
    "\n",
    "def predict_point(X, model, num_samples):\n",
    "    pred_dist = predict_dist(X, model, num_samples)\n",
    "    return pred_dist.mean(axis=1), pred_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-27T15:04:29.740438Z",
     "iopub.status.busy": "2022-04-27T15:04:29.740093Z",
     "iopub.status.idle": "2022-04-27T15:04:29.769486Z",
     "shell.execute_reply": "2022-04-27T15:04:29.768627Z",
     "shell.execute_reply.started": "2022-04-27T15:04:29.740409Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function PD: Calculate Poisson Deviance\n",
    "def PD_function(pred, obs):\n",
    "    PD = 200*( sum(pred) - sum(obs) + sum( np.log( (obs/pred)**(obs) ) ) )\n",
    "    return PD/len(pred)\n",
    "\n",
    "\n",
    "# evaluate a single mlp model\n",
    "def evaluate_model(trainX, trainy, train_expo):\n",
    "\n",
    "    first_input = Input(shape=(14, ))\n",
    "\n",
    "    first_dense_00 = Dense(50)(first_input)\n",
    "    act_dense_00 = Activation(\"sigmoid\")(first_dense_00)\n",
    "    act_dense_00 = BatchNormalization()(act_dense_00)\n",
    "    Dropout_dense_00 = Dropout(0.3)(act_dense_00)\n",
    "\n",
    "    first_dense_0 = Dense(25)(Dropout_dense_00)\n",
    "    act_dense_0 = Activation(\"sigmoid\")(first_dense_0)\n",
    "    act_dense_0 = BatchNormalization()(act_dense_0)\n",
    "    Dropout_dense_0 = Dropout(0.2)(act_dense_0)\n",
    "\n",
    "    first_dense_1 = Dense(15)(Dropout_dense_0)\n",
    "    act_dense_1 = Activation(\"sigmoid\")(first_dense_1)\n",
    "    act_dense_1 = BatchNormalization()(act_dense_1)\n",
    "    Dropout_dense_1 = Dropout(0.1)(act_dense_1)\n",
    "\n",
    "    first_dense_2 = Dense(8)(Dropout_dense_1)\n",
    "    act_dense_2 = Activation(\"sigmoid\")(first_dense_2)\n",
    "    act_dense_2 = BatchNormalization()(act_dense_2)\n",
    "    Dropout_dense_2 = Dropout(0.1)(act_dense_2)\n",
    "\n",
    "    first_dense_3 = Dense(1)(Dropout_dense_2)\n",
    "    act_dense_3 = Activation('linear')(first_dense_3)\n",
    "\n",
    "    second_input = Input(shape=(1, ))\n",
    "\n",
    "    second_dense_1 = Dense(1)(add([act_dense_3, second_input])) \n",
    "    merge_one = Activation('exponential')(second_dense_1) \n",
    "\n",
    "    model_tanh = Model(inputs=[first_input, second_input], outputs=merge_one)\n",
    "\n",
    "    model_tanh.compile(optimizer=Nadam(), loss='poisson')\n",
    "    \n",
    "    model_tanh.fit([trainX, train_expo], trainy, epochs=450, verbose=0,\n",
    "                   batch_size=10000\n",
    "                  )\n",
    "\n",
    "    return model_tanh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ", 81, 37 ,71, 28, 70, 54, 59, 17, 79"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ", 54, 42, 88, 29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-27T15:06:46.480807Z",
     "iopub.status.busy": "2022-04-27T15:06:46.480518Z",
     "iopub.status.idle": "2022-04-27T15:40:03.181128Z",
     "shell.execute_reply": "2022-04-27T15:40:03.179715Z",
     "shell.execute_reply.started": "2022-04-27T15:06:46.480777Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_poisson_10 = pd.DataFrame()\n",
    "df_poisson_20 = pd.DataFrame()\n",
    "df_poisson_30 = pd.DataFrame()\n",
    "df_poisson_40 = pd.DataFrame()\n",
    "\n",
    "table_name = [\"deviance_train\", \"deviance_test\", \"diff_train\", \"diff_test\"]\n",
    "\n",
    "\n",
    "for p in [62, 54, 42, 88, 29]:\n",
    "    \n",
    "    print(p)\n",
    "    \n",
    "    data_learn = pd.read_csv(r'C:\\Users\\KRASNIQ\\Documents\\These - Copie\\data\\data_learn_'+str(62)+'.csv')\n",
    "    data_test = pd.read_csv(r'C:\\Users\\KRASNIQ\\Documents\\These - Copie\\data\\data_test_'+str(62)+'.csv')\n",
    "    \n",
    "    nex_data = pd.concat([data_learn, data_test])\n",
    "    \n",
    "    data_learn_, data_test_ = train_test_split(nex_data, stratify=nex_data['ClaimNb'], test_size=0.1, random_state=p)\n",
    "    \n",
    "    data_learn_['Exposure'] = data_learn_['Exposure'].clip(upper=1)\n",
    "    data_test_['Exposure'] = data_test_['Exposure'].clip(upper=1)\n",
    "    \n",
    "    data_learn_['Area'] = data_learn_['Area'].map({\"A\": 1, \"B\": 2,\"C\": 3,\"D\": 4,\"E\": 5,\"F\": 6,})\n",
    "    data_test_['Area'] = data_test_['Area'].map({\"A\": 1, \"B\": 2,\"C\": 3,\"D\": 4,\"E\": 5,\"F\": 6,})\n",
    "    \n",
    "    data_learn_ = data_learn_.drop([\"Area_class_1\",\"Area_class_2\",\"Area_class_3\",\"Area_class_4\",'ClaimAmount', 'DrivAge', 'BonusMalus','VehBrand',\n",
    "                                       'Density', 'Region', 'VehAge', 'VehGas', \"VehGas_class_2\"], axis=1)\n",
    "\n",
    "    data_test_ = data_test_.drop([\"Area_class_1\",\"Area_class_2\",\"Area_class_3\",\"Area_class_4\",'ClaimAmount', 'DrivAge', 'BonusMalus','VehBrand',\n",
    "                                    'Density', 'Region', 'VehAge', 'VehGas', \"VehGas_class_2\"], axis=1)\n",
    "    \n",
    "    # train test split \n",
    "    X_train__, y_valid__ = train_test_split(data_learn_, stratify=data_learn_['ClaimNb'], test_size=0.1, random_state=p)\n",
    "    \n",
    "    y_train = np.array(X_train__.filter(['ClaimNb']))\n",
    "    log_exp_train= np.array(np.log(X_train__.filter(['Exposure'])))\n",
    "    x_train = X_train__.drop(columns=['ClaimNb', 'Exposure'])\n",
    "\n",
    "    y_valid = np.array(y_valid__.filter(['ClaimNb']))\n",
    "    log_exp_valid= np.array(np.log(y_valid__.filter(['Exposure'])))\n",
    "    x_valid = y_valid__.drop(columns=['ClaimNb', 'Exposure'])\n",
    "    \n",
    "    #valid\n",
    "    y_test = np.array(data_test_.filter(['ClaimNb']))\n",
    "    log_exp_test = np.array(np.log(data_test_.filter(['Exposure'])))\n",
    "    x_test = data_test_.drop(columns=['ClaimNb', 'Exposure'])\n",
    "    \n",
    "    #feature eng\n",
    "    cs = MinMaxScaler()\n",
    "    X_train = cs.fit_transform(x_train)\n",
    "    X_valid = cs.fit_transform(x_valid)\n",
    "    X_test = cs.transform(x_test)\n",
    "    \n",
    "    print(X_train.shape)\n",
    "    model = evaluate_model(X_train, y_train, log_exp_train)\n",
    "    \n",
    "    \n",
    "    ############################### strategie 1 ##########################################################\n",
    "    \n",
    "    print('strategie Drop Out')\n",
    "    \n",
    "\n",
    "\n",
    "    print('10')\n",
    "    predictions_test_10, pred_dist_test_10 = predict_point([X_test, log_exp_test], model, 10)\n",
    "    predictions_train_10, pred_dist_train_10 = predict_point([X_train, log_exp_train], model, 10)\n",
    "\n",
    "    #deviance\n",
    "    table_modal= [np.round(PD_function(predictions_train_10, y_train.flatten()),6),\n",
    "                  np.round(PD_function(predictions_test_10, y_test.flatten()),6),\n",
    "                  (sum(predictions_train_10) - sum(y_train.flatten()))/sum(y_train.flatten())*100,\n",
    "                  (sum(predictions_test_10) -sum(y_test.flatten()))/sum(y_test.flatten())*100]\n",
    "\n",
    "    cnt = dict()\n",
    "    k=0\n",
    "    for word in table_name:\n",
    "        cnt[word] = table_modal[k]\n",
    "        k = k+1\n",
    "    df_poisson_10 = df_poisson_10.append(cnt, ignore_index=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    print('20')    \n",
    "    predictions_test_20, pred_dist_test_20 = predict_point([X_test, log_exp_test], model, 20)\n",
    "    predictions_train_20, pred_dist_train_20 = predict_point([X_train, log_exp_train], model, 20)\n",
    "\n",
    "    #deviance\n",
    "    table_modal= [np.round(PD_function(predictions_train_20, y_train.flatten()),6),\n",
    "                  np.round(PD_function(predictions_test_20, y_test.flatten()),6),\n",
    "                  (sum(predictions_train_20) - sum(y_train.flatten()))/sum(y_train.flatten())*100,\n",
    "                  (sum(predictions_test_20) -sum(y_test.flatten()))/sum(y_test.flatten())*100]\n",
    "\n",
    "    cnt = dict()\n",
    "    k=0\n",
    "    for word in table_name:\n",
    "        cnt[word] = table_modal[k]\n",
    "        k = k+1\n",
    "    df_poisson_20 = df_poisson_20.append(cnt, ignore_index=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    print('30')\n",
    "    predictions_test_30, pred_dist_test_30 = predict_point([X_test, log_exp_test], model, 50)\n",
    "    predictions_train_30, pred_dist_train_30 = predict_point([X_train, log_exp_train], model, 50)\n",
    "\n",
    "    #deviance\n",
    "    table_modal= [np.round(PD_function(predictions_train_30, y_train.flatten()),6),\n",
    "                  np.round(PD_function(predictions_test_30, y_test.flatten()),6),\n",
    "                  (sum(predictions_train_30) - sum(y_train.flatten()))/sum(y_train.flatten())*100,\n",
    "                  (sum(predictions_test_30) -sum(y_test.flatten()))/sum(y_test.flatten())*100]\n",
    "\n",
    "    cnt = dict()\n",
    "    k=0\n",
    "    for word in table_name:\n",
    "        cnt[word] = table_modal[k]\n",
    "        k = k+1\n",
    "    df_poisson_30 = df_poisson_30.append(cnt, ignore_index=True)\n",
    "\n",
    "    \n",
    "    \n",
    "    print('40')\n",
    "    predictions_test_40, pred_dist_test_40 = predict_point([X_test, log_exp_test], model, 50)\n",
    "    predictions_train_40, pred_dist_train_40 = predict_point([X_train, log_exp_train], model, 50)\n",
    "\n",
    "    #deviance\n",
    "    table_modal= [np.round(PD_function(predictions_train_40, y_train.flatten()),6),\n",
    "                  np.round(PD_function(predictions_test_40, y_test.flatten()),6),\n",
    "                  (sum(predictions_train_40) - sum(y_train.flatten()))/sum(y_train.flatten())*100,\n",
    "                  (sum(predictions_test_40) -sum(y_test.flatten()))/sum(y_test.flatten())*100]\n",
    "\n",
    "    cnt = dict()\n",
    "    k=0\n",
    "    for word in table_name:\n",
    "        cnt[word] = table_modal[k]\n",
    "        k = k+1\n",
    "    df_poisson_40 = df_poisson_40.append(cnt, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_poisson_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_poisson_10[\"deviance_train\"].mean(), df_poisson_10[\"deviance_test\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_poisson_10[\"deviance_train\"].std(), df_poisson_10[\"deviance_test\"].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ensemble\", predictions_test_10[35])\n",
    "print(\"target\", y_test[35])\n",
    "sns.kdeplot(pred_dist_test_10[35], shade=True)\n",
    "plt.axvline(predictions_test_10[35], color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_poisson_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_poisson_20[\"deviance_train\"].mean(), df_poisson_20[\"deviance_test\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(30.117755999999996, 30.144133600000004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_poisson_20[\"deviance_train\"].std(), df_poisson_20[\"deviance_test\"].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(30.117755999999996, 30.144133600000004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ensemble\", predictions_test_20[35])\n",
    "print(\"target\", y_test[35])\n",
    "sns.kdeplot(pred_dist_test_20[35], shade=True)\n",
    "plt.axvline(predictions_test_20[35], color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_poisson_30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_poisson_30[\"deviance_train\"].mean(), df_poisson_30[\"deviance_test\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_poisson_30[\"deviance_train\"].std(), df_poisson_30[\"deviance_test\"].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ensemble\", predictions_test_30[35])\n",
    "print(\"target\", y_test[35])\n",
    "sns.kdeplot(pred_dist_test_30[35], shade=True)\n",
    "plt.axvline(predictions_test_30[35], color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_poisson_40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_poisson_40[\"deviance_train\"].mean(), df_poisson_40[\"deviance_test\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_poisson_40[\"deviance_train\"].std(), df_poisson_40[\"deviance_test\"].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ensemble\", predictions_test_40[35])\n",
    "print(\"target\", y_test[35])\n",
    "sns.kdeplot(pred_dist_test_40[35], shade=True)\n",
    "plt.axvline(predictions_test_40[35], color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
