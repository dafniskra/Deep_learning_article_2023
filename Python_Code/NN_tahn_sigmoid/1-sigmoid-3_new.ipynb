{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-27T15:04:29.693601Z",
     "iopub.status.busy": "2022-04-27T15:04:29.693321Z",
     "iopub.status.idle": "2022-04-27T15:04:29.700761Z",
     "shell.execute_reply": "2022-04-27T15:04:29.700069Z",
     "shell.execute_reply.started": "2022-04-27T15:04:29.693573Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import numpy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, add, BatchNormalization, concatenate, Embedding, Flatten \n",
    "from tensorflow.keras.optimizers import Nadam\n",
    "from keras.layers.core import Dropout, Activation\n",
    "from keras.initializers import RandomNormal, Ones\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import random\n",
    "from numpy import array\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function PD: Calculate Poisson Deviance\n",
    "def PD_function(pred, obs):\n",
    "    PD = 200*( sum(pred) - sum(obs) + sum( np.log( (obs/pred)**(obs) ) ) )\n",
    "    return PD/len(pred)\n",
    "    \n",
    "# evaluate a single mlp model\n",
    "def evaluate_model(trainX, trainy, train_expo, trainbrand, trainregion,\n",
    "                   activ_fonction_0, activ_fonction_1, activ_fonction_2,\n",
    "                   nb_neurones_0, nb_neurones_1, nb_neurones_2, \n",
    "                   drop_out_1, drop_out_2\n",
    "                  ):\n",
    "    \n",
    "    first_input = Input(shape=(7))\n",
    "    \n",
    "    brand_input = Input(shape=(1))\n",
    "    region_input = Input(shape=(1))    \n",
    "    \n",
    "    \n",
    "    Embedding_brand = Flatten()(Embedding(input_dim = 11, output_dim = 2)(brand_input))\n",
    "    Embedding_region = Flatten()(Embedding(input_dim = 22, output_dim = 2)(region_input))\n",
    "    \n",
    "    \n",
    "    merge_one_ = tf.keras.layers.Concatenate()([Embedding_brand, Embedding_region, first_input])\n",
    "    \n",
    "    first_dense_0 = Dense(nb_neurones_0)(merge_one_)\n",
    "    act_dense_0 = Activation(activ_fonction_0)(first_dense_0)\n",
    "    \n",
    "    first_dense_1 = Dense(nb_neurones_1)(act_dense_0)\n",
    "    act_dense_1 = Activation(activ_fonction_1)(first_dense_1)\n",
    "    \n",
    "    first_dense_2 = Dense(nb_neurones_2)(act_dense_1)\n",
    "    act_dense_2 = Activation(activ_fonction_2)(first_dense_2)\n",
    "\n",
    "    first_dense_3 = Dense(1, \n",
    "                          bias_initializer=tf.keras.initializers.Constant(value=-2.29881790925729), \n",
    "                          kernel_initializer=tf.keras.initializers.Zeros())(act_dense_2)\n",
    "    act_dense_3 = Activation('linear')(first_dense_3)\n",
    "\n",
    "    \n",
    "    \n",
    "    second_input = Input(shape=(1, ))\n",
    "\n",
    "    second_dense_1 = Dense(1, \n",
    "                           bias_initializer=tf.keras.initializers.Constant(value=0), \n",
    "                           kernel_initializer=tf.keras.initializers.Ones(), \n",
    "                           trainable=False)(add([act_dense_3, second_input])) \n",
    "    \n",
    "    outputt = Activation('exponential')(second_dense_1) \n",
    "\n",
    "    model_ = Model(inputs=[first_input, second_input, brand_input, region_input], outputs=outputt)\n",
    "\n",
    "    model_.compile(optimizer=Nadam(), loss='poisson')\n",
    "    \n",
    "    model_.fit([trainX, train_expo, trainbrand, trainregion], trainy, epochs=450, verbose=0,\n",
    "                   batch_size=10000\n",
    "                  )\n",
    "\n",
    "    #preds = model_.predict([testX, test_expo, testbrand, testregion]).flatten()  \n",
    "\n",
    "    return model_, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-27T15:04:29.740438Z",
     "iopub.status.busy": "2022-04-27T15:04:29.740093Z",
     "iopub.status.idle": "2022-04-27T15:04:29.769486Z",
     "shell.execute_reply": "2022-04-27T15:04:29.768627Z",
     "shell.execute_reply.started": "2022-04-27T15:04:29.740409Z"
    }
   },
   "outputs": [],
   "source": [
    "# make an ensemble prediction for multi-class classification\n",
    "def ensemble_predictions(members, X_test, expo_test, brand_test, region_test):\n",
    "    \n",
    "    # make predictions\n",
    "    yhats = [model.predict([X_test, expo_test, brand_test, region_test]) for model in members]\n",
    "    yhats = array(yhats)\n",
    "    \n",
    "    result = np.mean(yhats, axis=0).flatten() \n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "# evaluate a specific number of members in an ensemble\n",
    "def evaluate_n_members_bis(members, testX, testexpo, testbrand, testregion):\n",
    "    \n",
    "    # select a subset of members\n",
    "    subset = members[:len(members)]\n",
    "    \n",
    "    # make prediction\n",
    "    yhat = ensemble_predictions(subset, testX, testexpo, testbrand, testregion)\n",
    "    \n",
    "    # calculate accuracy \n",
    "    return yhat\n",
    "\n",
    "\n",
    "def Ensemble_fit(Xtrain_, ytrain_, expotrain_, brandtrain_, regiontrain_,\n",
    "                 samples=30, rate_OOB=0.2, type_ = \"Bagging\", ver_freq_train=True):\n",
    "    \n",
    "    scores_, members_ = list(), list()\n",
    "    \n",
    "    activ_fonction_0, activ_fonction_1, activ_fonction_2 = random.sample([\"sigmoid\"], 1), random.sample([\"sigmoid\"], 1), random.sample([\"sigmoid\"], 1)\n",
    "    nb_neurones_0, nb_neurones_1, nb_neurones_2 = random.sample([25], 1), random.sample([15], 1), random.sample([10], 1)\n",
    "    drop_out_1, drop_out_2 = random.sample([0.02, 0.04, 0.06, 0.08], 1), random.sample([0.02, 0.04, 0.06, 0.08], 1)\n",
    "        \n",
    "    # evaluate model\n",
    "    model, test_acc = evaluate_model(Xtrain_, ytrain_, expotrain_, brandtrain_, regiontrain_, \n",
    "                                         activ_fonction_0[0], activ_fonction_1[0], activ_fonction_2[0], \n",
    "                                         nb_neurones_0[0], nb_neurones_1[0], nb_neurones_2[0], \n",
    "                                         drop_out_1[0], drop_out_2[0])\n",
    "        \n",
    "    print('> OOB > %.3f' % test_acc)\n",
    "    scores_.append(test_acc)\n",
    "    members_.append(model)   \n",
    "    \n",
    "    return members_, scores_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ", 81, 37 ,71, 28, 70, 54, 59, 17, 79"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-27T15:06:46.480807Z",
     "iopub.status.busy": "2022-04-27T15:06:46.480518Z",
     "iopub.status.idle": "2022-04-27T15:40:03.181128Z",
     "shell.execute_reply": "2022-04-27T15:40:03.179715Z",
     "shell.execute_reply.started": "2022-04-27T15:06:46.480777Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62\n",
      "> OOB > 0.000\n",
      "2119/2119 [==============================] - 2s 963us/step\n",
      "17162/17162 [==============================] - 18s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KRASNIQ\\AppData\\Local\\Temp\\ipykernel_20524\\540695334.py:91: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_poisson = df_poisson.append(cnt, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n",
      "> OOB > 0.000\n",
      "2119/2119 [==============================] - 4s 2ms/step\n",
      "17162/17162 [==============================] - 21s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KRASNIQ\\AppData\\Local\\Temp\\ipykernel_20524\\540695334.py:91: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_poisson = df_poisson.append(cnt, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n",
      "> OOB > 0.000\n",
      "2119/2119 [==============================] - 2s 942us/step\n",
      "17162/17162 [==============================] - 15s 901us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KRASNIQ\\AppData\\Local\\Temp\\ipykernel_20524\\540695334.py:91: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_poisson = df_poisson.append(cnt, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88\n",
      "> OOB > 0.000\n",
      "2119/2119 [==============================] - 2s 932us/step\n",
      "17162/17162 [==============================] - 16s 921us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KRASNIQ\\AppData\\Local\\Temp\\ipykernel_20524\\540695334.py:91: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_poisson = df_poisson.append(cnt, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n",
      "> OOB > 0.000\n",
      "2119/2119 [==============================] - 2s 930us/step\n",
      "17162/17162 [==============================] - 16s 940us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KRASNIQ\\AppData\\Local\\Temp\\ipykernel_20524\\540695334.py:91: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_poisson = df_poisson.append(cnt, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "df_poisson = pd.DataFrame()\n",
    "\n",
    "table_name = [\"deviance_train\", \"deviance_test\", \"diff_train\", \"diff_test\"]\n",
    "\n",
    "for p in [62, 54, 42, 88, 29]:\n",
    "    \n",
    "    print(p)\n",
    "    \n",
    "    data_learn = pd.read_csv(r'C:\\Users\\KRASNIQ\\Documents\\These_1.1\\data\\data_learn_'+str(62)+'.csv')\n",
    "    data_test = pd.read_csv(r'C:\\Users\\KRASNIQ\\Documents\\These_1.1\\data\\data_test_'+str(62)+'.csv')\n",
    "    \n",
    "    nex_data = pd.concat([data_learn, data_test])\n",
    "    \n",
    "    data_learn_, data_test_ = train_test_split(nex_data, stratify=nex_data['ClaimNb'], test_size=0.1, random_state=p)\n",
    "    \n",
    "    data_learn_['Exposure'] = data_learn_['Exposure'].clip(upper=1)\n",
    "    data_test_['Exposure'] = data_test_['Exposure'].clip(upper=1)\n",
    "    \n",
    "    data_learn_['Area'] = data_learn_['Area'].map({\"A\": 1, \"B\": 2,\"C\": 3,\"D\": 4,\"E\": 5,\"F\": 6,})\n",
    "    data_test_['Area'] = data_test_['Area'].map({\"A\": 1, \"B\": 2,\"C\": 3,\"D\": 4,\"E\": 5,\"F\": 6,})\n",
    "    \n",
    "    data_learn_ = data_learn_.drop([\"Area_class_1\",\"Area_class_2\",\"Area_class_3\",\"Area_class_4\",'ClaimAmount', 'DrivAge', 'BonusMalus',\n",
    "                                    \"VehBrand_class_1\", \"VehBrand_class_2\", \"VehBrand_class_3\",\n",
    "                                    'Density',\"Region_class_1\", \"Region_class_2\", \"Region_class_3\", \"Region_class_4\" , \n",
    "                                    'VehAge', 'VehGas', \"VehGas_class_2\"], axis=1)\n",
    "\n",
    "    data_test_ = data_test_.drop([\"Area_class_1\",\"Area_class_2\",\"Area_class_3\",\"Area_class_4\",'ClaimAmount', 'DrivAge', 'BonusMalus',\n",
    "                                  \"VehBrand_class_1\", \"VehBrand_class_2\", \"VehBrand_class_3\",\n",
    "                                  'Density',\"Region_class_1\", \"Region_class_2\", \"Region_class_3\", \"Region_class_4\" , \n",
    "                                  'VehAge', 'VehGas', \"VehGas_class_2\"], axis=1)\n",
    "    \n",
    "    # train test split \n",
    "    X_train__, X_valid__ = train_test_split(data_learn_, stratify=data_learn_['ClaimNb'], test_size=0.1, random_state=p)\n",
    "    \n",
    "    \n",
    "    #################### label encoding\n",
    "    \n",
    "    le_region = preprocessing.LabelEncoder()\n",
    "    le_region.fit(X_train__['Region'])\n",
    "\n",
    "    le_brand = preprocessing.LabelEncoder()\n",
    "    le_brand.fit(X_train__['VehBrand'])\n",
    "    \n",
    "    y_train = np.array(X_train__.filter(['ClaimNb']))\n",
    "    log_exp_train= np.array(np.log(X_train__.filter(['Exposure'])))\n",
    "    region_train = np.array(le_region.transform(X_train__['Region']))\n",
    "    brand_train = np.array(le_brand.transform(X_train__['VehBrand']))\n",
    "    x_train = X_train__.drop(columns=['ClaimNb', 'Exposure', 'VehBrand', 'Region'])\n",
    "\n",
    "    y_valid = np.array(X_valid__.filter(['ClaimNb']))\n",
    "    log_exp_valid= np.array(np.log(X_valid__.filter(['Exposure'])))\n",
    "    region_valid = np.array(le_region.transform(X_valid__['Region']))\n",
    "    brand_valid = np.array(le_brand.transform(X_valid__['VehBrand']))\n",
    "    x_valid = X_valid__.drop(columns=['ClaimNb', 'Exposure', 'VehBrand', 'Region'])\n",
    "    \n",
    "    #valid\n",
    "    y_test = np.array(data_test_.filter(['ClaimNb']))\n",
    "    log_exp_test = np.array(np.log(data_test_.filter(['Exposure'])))\n",
    "    region_test = np.array(le_region.transform(data_test_['Region']))\n",
    "    brand_test = np.array(le_brand.transform(data_test_['VehBrand']))\n",
    "    x_test = data_test_.drop(columns=['ClaimNb', 'Exposure', 'VehBrand', 'Region'])\n",
    "    \n",
    "    #feature eng\n",
    "    cs = MinMaxScaler()\n",
    "    X_train = np.array(2*((x_train - x_train.min())/(x_train.max()-x_train.min())) - 1)\n",
    "    X_valid = np.array(2*((x_valid - x_train.min())/(x_train.max()-x_train.min())) - 1)\n",
    "    X_test = np.array(2*((x_test - x_train.min())/(x_train.max()-x_train.min())) - 1)\n",
    "    \n",
    "    \n",
    "    members, scores = Ensemble_fit(X_train, y_train, log_exp_train, brand_train, region_train,\n",
    "                                   samples=1, rate_OOB=0, type_ = \"Bagging\",\n",
    "                                   ver_freq_train=False)\n",
    "\n",
    "    \n",
    "    predictions_test = evaluate_n_members_bis(members, X_test, log_exp_test, brand_test, region_test)\n",
    "    predictions_train = evaluate_n_members_bis(members, X_train, log_exp_train, brand_train, region_train)\n",
    "\n",
    "    #deviance\n",
    "    table_modal= [np.round(PD_function(predictions_train, y_train.flatten()),6),\n",
    "                  np.round(PD_function(predictions_test, y_test.flatten()),6),\n",
    "                  (sum(predictions_train) - sum(y_train.flatten()))/sum(y_train.flatten())*100,\n",
    "                  (sum(predictions_test) -sum(y_test.flatten()))/sum(y_test.flatten())*100]\n",
    "    \n",
    "    table_name = [\"deviance_train\", \"deviance_test\", \"diff_train\", \"diff_test\"]\n",
    "\n",
    "    cnt = dict()\n",
    "    k=0\n",
    "    for word in table_name:\n",
    "        cnt[word] = table_modal[k]\n",
    "        k = k+1\n",
    "    df_poisson = df_poisson.append(cnt, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-27T15:40:03.182302Z",
     "iopub.status.idle": "2022-04-27T15:40:03.182774Z",
     "shell.execute_reply": "2022-04-27T15:40:03.182562Z",
     "shell.execute_reply.started": "2022-04-27T15:40:03.182534Z"
    }
   },
   "outputs": [],
   "source": [
    "df_poisson['model']=[\"cv-bagging-1\"]*len(df_poisson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-27T15:05:28.797735Z",
     "iopub.status.idle": "2022-04-27T15:05:28.798055Z",
     "shell.execute_reply": "2022-04-27T15:05:28.797915Z",
     "shell.execute_reply.started": "2022-04-27T15:05:28.797878Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>deviance_train</th>\n",
       "      <th>deviance_test</th>\n",
       "      <th>diff_train</th>\n",
       "      <th>diff_test</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31.006845</td>\n",
       "      <td>31.084953</td>\n",
       "      <td>2.198838</td>\n",
       "      <td>2.089831</td>\n",
       "      <td>cv-bagging-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31.060857</td>\n",
       "      <td>30.889755</td>\n",
       "      <td>-0.869387</td>\n",
       "      <td>-0.658852</td>\n",
       "      <td>cv-bagging-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31.030274</td>\n",
       "      <td>30.794989</td>\n",
       "      <td>-3.803919</td>\n",
       "      <td>-4.055198</td>\n",
       "      <td>cv-bagging-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30.991696</td>\n",
       "      <td>31.127448</td>\n",
       "      <td>0.309246</td>\n",
       "      <td>0.301302</td>\n",
       "      <td>cv-bagging-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30.966444</td>\n",
       "      <td>31.173025</td>\n",
       "      <td>0.566817</td>\n",
       "      <td>0.731398</td>\n",
       "      <td>cv-bagging-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   deviance_train  deviance_test  diff_train  diff_test         model\n",
       "0       31.006845      31.084953    2.198838   2.089831  cv-bagging-1\n",
       "1       31.060857      30.889755   -0.869387  -0.658852  cv-bagging-1\n",
       "2       31.030274      30.794989   -3.803919  -4.055198  cv-bagging-1\n",
       "3       30.991696      31.127448    0.309246   0.301302  cv-bagging-1\n",
       "4       30.966444      31.173025    0.566817   0.731398  cv-bagging-1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_poisson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-27T15:05:28.799311Z",
     "iopub.status.idle": "2022-04-27T15:05:28.799679Z",
     "shell.execute_reply": "2022-04-27T15:05:28.799533Z",
     "shell.execute_reply.started": "2022-04-27T15:05:28.799515Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31.0112232, 31.014034000000002, -0.31968084425539545, -0.3183039660851977)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_poisson[\"deviance_train\"].mean(), df_poisson[\"deviance_test\"].mean(), df_poisson[\"diff_train\"].mean(), df_poisson[\"diff_test\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.03616664212641206,\n",
       " 0.16324548991013432,\n",
       " 2.234192138442817,\n",
       " 2.3111280447977394)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_poisson[\"deviance_train\"].std(), df_poisson[\"deviance_test\"].std(), df_poisson[\"diff_train\"].std(), df_poisson[\"diff_test\"].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
